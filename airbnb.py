# -*- coding: utf-8 -*-
"""AirBnb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/181y3zHMjhLHXcaKSML6sbo7JRSCws4KT
"""

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/airbnb.csv'  # Change this to your actual file path
#df = pd.read_csv(file_path)
df = pd.read_csv(file_path, na_values=['', ' ', 'N/A', 'NA', 'None'])

# Display first 5 rows
df.head()

# Check data types and non-null counts
df.info()

# Check for missing values
missing_values = df.isnull().sum()
missing_values[missing_values > 0]

import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np

# Apply log transformation to handle large values
df['log_price'] = np.log1p(df['price'])  # log1p to handle values >=0

# Plot the log-transformed price distribution
plt.figure(figsize=(10, 5))
sns.histplot(df['log_price'], kde=True, bins=50)
plt.title('Log-Transformed Price Distribution')
plt.xlabel('Log(Price + 1)')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 5))
sns.histplot(df['price'], bins=100, kde=True)
plt.title('Price Distribution (Detailed Bins)')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(x=df['price'])
plt.title('Detailed Boxplot of Price')
plt.xlabel('Price')
plt.show()

print(df.dtypes)
# dropping rows with missing ratings
df.dropna(subset=['rating'], inplace=True)
# Clean columns that should be numeric (e.g., price, reviews)
df['reviews'] = pd.to_numeric(df['reviews'], errors='coerce')
df['price'] = pd.to_numeric(df['price'], errors='coerce')
# Ensure 'rating', 'bathrooms', 'beds', 'guests', etc., are in numeric format (if not already)
df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
df['bathrooms'] = pd.to_numeric(df['bathrooms'], errors='coerce')
df['beds'] = pd.to_numeric(df['beds'], errors='coerce')
df['guests'] = pd.to_numeric(df['guests'], errors='coerce')
df['toiles'] = pd.to_numeric(df['toiles'], errors='coerce')
df['bedrooms'] = pd.to_numeric(df['bedrooms'], errors='coerce')
df['studios'] = pd.to_numeric(df['studios'], errors='coerce')

# Handling the missing values in the reviews column
df['reviews'].fillna(0, inplace=True)


numerical_features = ['rating', 'reviews', 'bathrooms', 'beds', 'guests', 'toiles', 'bedrooms', 'studios', 'price']
print(df[numerical_features].isnull().sum())  # Check for missing values

corr_matrix = df[numerical_features].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title("Correlation Matrix")
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='country', order=df['country'].value_counts().index)
plt.title('Distribution of Country')
plt.xticks(rotation=90)
plt.show()

# Visualizing 'bedrooms' distribution
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='bedrooms', order=df['bedrooms'].value_counts().index)
plt.title('Distribution of Bedrooms')
plt.show()

# Visualizing 'host_name' distribution
top_hosts = df['host_name'].value_counts().head(10).index
plt.figure(figsize=(10, 6))
sns.countplot(data=df[df['host_name'].isin(top_hosts)], x='host_name', order=top_hosts)
plt.title('Top 10 Hosts Distribution')
plt.xticks(rotation=90)
plt.show()

# Boxplot for 'price' vs 'country'
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='country', y='price')
plt.title('Price Distribution by Country')
plt.xticks(rotation=90)
plt.show()

# Boxplot for 'price' vs 'bedrooms'
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='bedrooms', y='price')
plt.title('Price Distribution by Bedrooms')
plt.show()

# Boxplot for 'price' vs 'host_name' (Top 10 hosts)
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[df['host_name'].isin(top_hosts)], x='host_name', y='price')
plt.title('Price Distribution by Host Name (Top 10 Hosts)')
plt.xticks(rotation=90)
plt.show()

missing_values = df.isnull().sum()
print("Missing values in each column:\n", missing_values)
df.dropna(subset=['price'], inplace=True)

df['reviews'].fillna(df['reviews'].median(), inplace=True)  # Impute missing reviews with median value
df['rating'].fillna(df['rating'].mean(), inplace=True)  # Impute missing ratings with the mean value

# Checking for missing values
missing_values_after = df.isnull().sum()
print("\nMissing values after handling:\n", missing_values_after)

plt.figure(figsize=(10, 6))
sns.boxplot(df['price'])
plt.title('Boxplot for Price')
plt.show()

# Detecting outliers
Q1 = df['price'].quantile(0.25)
Q3 = df['price'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[(df['price'] < (Q1 - 1.5 * IQR)) | (df['price'] > (Q3 + 1.5 * IQR))]
print(f"Outliers detected: {outliers.shape[0]} rows")
df = df[(df['price'] >= (Q1 - 1.5 * IQR)) & (df['price'] <= (Q3 + 1.5 * IQR))]
print(f"Dataset after removing outliers: {df.shape[0]} rows")

print("Columns in the DataFrame: ", df.columns)

import pandas as pd

# One-hot encode
cols_to_encode = [col for col in ['host_name', 'country'] if col in df.columns]
if cols_to_encode:
    df = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)
else:
    print(f"Skipped encoding: columns {['host_name', 'country']} not found in DataFrame.")
if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

sparse_columns = [col for col in df.select_dtypes(include=['number', 'bool']).columns if df[col].sum() == 0]
df.drop(columns=sparse_columns, inplace=True)

print(f"Dropped sparse columns: {sparse_columns}")
print(f"Data shape after encoding and cleanup: {df.shape}")
print(df.head())

from sklearn.preprocessing import StandardScaler

numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

scaler = StandardScaler()

df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

# Check the result
print(f"Scaled numeric columns: {numeric_cols}")
print(df.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPRegressor

target_column = 'price'

if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

X = df.drop(columns=[target_column])
y = df[target_column]

categorical_features = [col for col in X.select_dtypes(include=['object']).columns if col != target_column]
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),

        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), categorical_features)
    ])

model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', MLPRegressor(hidden_layer_sizes=(64, 64), max_iter=500))  # MLPRegressor for regression
])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model.fit(X_train, y_train)

score = model.score(X_test, y_test)
print(f"Model R^2 Score: {score:.4f}")

y_pred = model.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"R² Score: {r2:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score

target_column = 'price'
if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

X = df.drop(columns=[target_column])
y = df[target_column]

categorical_features = [col for col in X.select_dtypes(include=['object']).columns]
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler())
        ]), numerical_features),

        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ]), categorical_features)
    ]
)

model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', MLPRegressor(max_iter=300, random_state=42))  # faster convergence
])

param_grid = {
    'regressor__hidden_layer_sizes': [(64,), (64, 32)],
    'regressor__activation': ['relu', 'tanh'],
    'regressor__learning_rate_init': [0.001, 0.01],
}

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_small = X_train[:500]
y_small = y_train[:500]

search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=4,
    cv=3,
    scoring='r2',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

search.fit(X_small, y_small)

print("Best Parameters:", search.best_params_)
print("Best R² Score on small training subset:", search.best_score_)
best_model = search.best_estimator_
y_pred = best_model.predict(X_test)
print("Final R² on Test Set:", r2_score(y_test, y_pred))

from sklearn.model_selection import cross_val_score
import numpy as np

X_sample = X.sample(n=5000, random_state=42)
y_sample = y.loc[X_sample.index]

cv_scores = cross_val_score(model, X_sample, y_sample, cv=3, scoring='r2')

print("Cross-Validation R² Scores:", cv_scores)
print("Average R²:", np.mean(cv_scores))

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd

if 'Unnamed: 0' in df.columns:
    df.drop(columns=['Unnamed: 0'], inplace=True)

X = df.drop(columns=[target_column])
y = df[target_column]

categorical_features = X.select_dtypes(include='object').columns.tolist()
numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()

preprocessor = ColumnTransformer(transformers=[
    ('num', SimpleImputer(strategy='mean'), numerical_features),
    ('cat', Pipeline([
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ]), categorical_features)
])

X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_processed = preprocessor.fit_transform(X_train_raw)
X_test_processed = preprocessor.transform(X_test_raw)

rf = RandomForestRegressor(random_state=42, n_estimators=50, n_jobs=-1)  # fewer trees for speed
gb = GradientBoostingRegressor(random_state=42, n_estimators=50)         # fewer trees

rf.fit(X_train_processed, y_train)
gb.fit(X_train_processed, y_train)

rf_preds = rf.predict(X_test_processed)
gb_preds = gb.predict(X_test_processed)
ensemble_preds = (rf_preds + gb_preds) / 2

r2_rf = r2_score(y_test, rf_preds)
r2_gb = r2_score(y_test, gb_preds)
r2_ensemble = r2_score(y_test, ensemble_preds)

print(f"Random Forest R²: {r2_rf:.4f}")
print(f"Gradient Boosting R²: {r2_gb:.4f}")
print(f"Ensemble R²: {r2_ensemble:.4f}")